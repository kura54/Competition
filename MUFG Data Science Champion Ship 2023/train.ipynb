{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPTk7D0vRVqb"
      },
      "source": [
        "# Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uWNzDf7RVqd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import datetime\n",
        "from sklearn.cluster import KMeans\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import japanize_matplotlib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "import lightgbm as lgb\n",
        "import joblib\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import time\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import optuna\n",
        "from sklearn.metrics import roc_auc_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76B9hUggRVqf"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OSXryGhRVqf"
      },
      "outputs": [],
      "source": [
        "train_path = \"D:\\\\MUFG Data Science Champion Ship 2023\\\\train.csv\"\n",
        "df_train = pd.read_csv(train_path)\n",
        "\n",
        "card_path = \"D:\\\\MUFG Data Science Champion Ship 2023\\\\card.csv\"\n",
        "df_card = pd.read_csv(card_path)\n",
        "\n",
        "user_path = \"D:\\\\MUFG Data Science Champion Ship 2023\\\\user.csv\"\n",
        "df_user = pd.read_csv(user_path)\n",
        "\n",
        "test_path = \"D:\\\\MUFG Data Science Champion Ship 2023\\\\test.csv\"\n",
        "df_test = pd.read_csv(test_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGNJqiERRVqf"
      },
      "source": [
        "## Merge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylt9CVBwRVqg"
      },
      "outputs": [],
      "source": [
        "# is_testカラムを追加\n",
        "df_train['is_test'] = 0\n",
        "df_test['is_test'] = 1\n",
        "\n",
        "# データフレームをマージ\n",
        "merged_df1 = pd.concat([df_train, df_test], axis=0, ignore_index=True)\n",
        "\n",
        "merged_df2 = merged_df1.merge(df_user, on='user_id', how='left')\n",
        "\n",
        "final_data = merged_df2.merge(df_card, on=['user_id', 'card_id'], how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqV3yLNbRVqg"
      },
      "outputs": [],
      "source": [
        "print(df_train.shape)\n",
        "print(df_test.shape)\n",
        "print(df_user.shape)\n",
        "print(df_card.shape)\n",
        "print(merged_df1.shape)\n",
        "print(merged_df2.shape)\n",
        "print(final_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKKu6vKXRVqh"
      },
      "outputs": [],
      "source": [
        "final_data.to_csv('D:\\\\MUFG Data Science Champion Ship 2023\\\\output\\\\output2.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJA9Af1zRVqh"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.expand_frame_repr', False)\n",
        "\n",
        "final_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lezmGi19RVqh"
      },
      "source": [
        "## 前処理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHmplA-wRVqh"
      },
      "outputs": [],
      "source": [
        "final_data['amount'] = final_data['amount'].str.replace('$', '').astype(float)\n",
        "final_data['credit_limit'] = final_data['credit_limit'].str.replace('$', '').astype(float)\n",
        "final_data['per_capita_income_zipcode'] = final_data['per_capita_income_zipcode'].str.replace('$', '').astype(float)\n",
        "final_data['yearly_income_person'] = final_data['yearly_income_person'].str.replace('$', '').astype(float)\n",
        "final_data['total_debt'] = final_data['total_debt'].str.replace('$', '').astype(float)\n",
        "final_data.drop('index', axis=1, inplace=True)\n",
        "final_data.drop('user_id', axis=1, inplace=True)\n",
        "# final_data.drop('merchant_city', axis=1, inplace=True)\n",
        "\n",
        "# Check the first few rows to confirm the change\n",
        "final_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbNzZDqrRVqi"
      },
      "source": [
        "# Drop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atVlqhmYRVqi"
      },
      "outputs": [],
      "source": [
        "# # 削除するカラムのリスト\n",
        "# columns_to_drop = [\n",
        "#     'card_brand', 'state', 'has_chip', 'gender',\n",
        "#     'cards_issued', 'card_type', 'num_credit_cards',\n",
        "#     'birth_year', 'yearly_income_person', 'retirement_age', 'index', 'user_id', 'address', 'birth_month'\n",
        "# ]\n",
        "\n",
        "# カラムを削除\n",
        "final_data = final_data.drop(columns=columns_to_drop)\n",
        "\n",
        "# Check the first few rows to confirm the change\n",
        "final_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQGgDJtWRVqi"
      },
      "outputs": [],
      "source": [
        "print(lgb.__version__)\n",
        "print(final_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bg1nJwnVRVqi"
      },
      "source": [
        "# 学習"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Le3euXDlRVqi"
      },
      "outputs": [],
      "source": [
        "# カテゴリ変数を指定\n",
        "categorical_features = final_data.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# カテゴリ変数をカテゴリ型に変換\n",
        "for col in categorical_features:\n",
        "    final_data[col] = final_data[col].astype('category')\n",
        "\n",
        "# `is_test` カラムを使用して学習用データとテストデータに分割\n",
        "final_data_train = final_data[final_data['is_test'] == 0]\n",
        "final_data_test = final_data[final_data['is_test'] == 1]\n",
        "\n",
        "X = final_data_train.drop(columns=['is_fraud?', 'is_test'])\n",
        "y = final_data_train['is_fraud?']\n",
        "\n",
        "# ハイパーパラメータの設定\n",
        "params = {\n",
        "    'objective': 'binary',\n",
        "    'metric': 'binary_logloss',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 31,\n",
        "    'learning_rate': 0.05,\n",
        "    'feature_fraction': 0.9\n",
        "}\n",
        "\n",
        "# 4-fold クロスバリデーションを実行\n",
        "folds = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
        "f1_scores = []\n",
        "optimal_thresholds = []  # 各フォールドの最適な閾値を保存するリスト\n",
        "\n",
        "for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n",
        "    print(f'Fold {fold_n + 1} started at {time.ctime()}')\n",
        "\n",
        "    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
        "    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
        "\n",
        "    train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=categorical_features)\n",
        "    valid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=categorical_features, reference=train_data)\n",
        "\n",
        "    num_round = 1000\n",
        "    bst = lgb.train(params, train_data, num_round, valid_sets=[valid_data])\n",
        "\n",
        "     # 予測確率を取得\n",
        "    y_pred_probs = bst.predict(X_valid, num_iteration=bst.best_iteration)\n",
        "    # 適合率、再現率、閾値を計算\n",
        "    precision, recall, thresholds = precision_recall_curve(y_valid, y_pred_probs)\n",
        "    # F1スコアを計算\n",
        "    f1_scores_thresholds = 2 * (precision * recall) / (precision + recall)\n",
        "    # 最大のF1スコアを持つ閾値を取得\n",
        "    optimal_threshold = thresholds[np.argmax(f1_scores_thresholds)]\n",
        "    optimal_thresholds.append(optimal_threshold)\n",
        "    # 予測確率をバイナリの0 or 1に変換\n",
        "    y_pred_binary = (y_pred_probs > optimal_threshold).astype(int)\n",
        "\n",
        "    f1 = f1_score(y_valid, y_pred_binary)\n",
        "    f1_scores.append(f1)\n",
        "\n",
        "    # モデルを保存\n",
        "    bst.save_model(f'D:\\\\MUFG Data Science Champion Ship 2023\\\\model\\\\model_fold{fold_n + 1}.txt')\n",
        "\n",
        "print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(f1_scores), np.std(f1_scores)))\n",
        "#58.2s\n",
        "#CV:0.6348"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUXHIm7aRVqj"
      },
      "source": [
        "# test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o09kxUjiRVqj"
      },
      "outputs": [],
      "source": [
        "# テストデータでの予測\n",
        "predictions = []\n",
        "\n",
        "# テストデータの特徴量を取得\n",
        "final_data_test_features = final_data_test.drop(columns=['is_fraud?', 'is_test'])\n",
        "\n",
        "# 各モデルでの予測を行う\n",
        "for fold_n in range(4):\n",
        "    bst = lgb.Booster(model_file=f'model_fold{fold_n + 1}.txt')\n",
        "    preds = bst.predict(final_data_test_features, num_iteration=bst.best_iteration)\n",
        "    predictions.append(preds)\n",
        "\n",
        "# 予測の平均を取得\n",
        "mean_preds = np.mean(predictions, axis=0)\n",
        "\n",
        "# 4つの最適な閾値の平均を取得\n",
        "mean_optimal_threshold = np.mean(optimal_thresholds)\n",
        "\n",
        "# 予測結果を二値化\n",
        "binary_predictions = (mean_preds > mean_optimal_threshold).astype(int)\n",
        "\n",
        "# 提出用のデータフレームを作成\n",
        "submission_df = pd.DataFrame({\n",
        "    'ID': df_test['index'].values,\n",
        "    'is_fraud?': binary_predictions\n",
        "})\n",
        "\n",
        "# ヘッダーとインデックスを無効にしてCSVに保存\n",
        "submission_df.to_csv('submission.csv', index=False, header=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtnIcDqmRVqo"
      },
      "outputs": [],
      "source": [
        "submission_df.to_csv('D:\\\\MUFG Data Science Champion Ship 2023\\\\predictions\\\\submit_baseline1.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Udngjup1RVqv"
      },
      "source": [
        "# 特徴量需要度"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcvdMUgCRVqv"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import shap\n",
        "import seaborn as sns\n",
        "\n",
        "# Get feature importances from the model\n",
        "feature_importances = bst.feature_importance() #importance_type='gain'\n",
        "\n",
        "# Create a DataFrame for visualization\n",
        "features_df = pd.DataFrame({\n",
        "    'Feature': X.columns,  # Ensure this is the complete feature set used for training\n",
        "    'Importance': feature_importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Plot feature importances\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.barplot(x='Importance', y='Feature', data=features_df)\n",
        "plt.title('Feature Importances')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLvTsA4ARVqv"
      },
      "outputs": [],
      "source": [
        "# Get feature importances from the model\n",
        "feature_importances = bst_lgb.feature_importance() #importance_type='gain'\n",
        "\n",
        "# Create a DataFrame for visualization\n",
        "features_df = pd.DataFrame({\n",
        "    'Feature': X.columns,  # Ensure this is the complete feature set used for training\n",
        "    'Importance': feature_importances\n",
        "}).sort_values(by='Importance', ascending=True)  # Change to ascending order for bottom 10\n",
        "\n",
        "# Print the bottom 10 features\n",
        "print(features_df.head(10))\n",
        "\n",
        "# If you still want to visualize all feature importances, you can sort again\n",
        "features_df = features_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Plot feature importances\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.barplot(x='Importance', y='Feature', data=features_df)\n",
        "plt.title('Feature Importances')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnx9VHCWRVqv"
      },
      "outputs": [],
      "source": [
        "# 1つ目のモデルをロード (ここでは1つのfoldを例として使用)\n",
        "bst = lgb.Booster(model_file='D:\\\\MUFG Data Science Champion Ship 2023\\\\model\\\\lgb_model_fold1.txt')\n",
        "\n",
        "# SHAPのExplainerを作成し、SHAP値を計算\n",
        "explainer = shap.TreeExplainer(bst)\n",
        "shap_values = explainer.shap_values(X_valid)\n",
        "\n",
        "# summary_plotの表示\n",
        "shap.summary_plot(shap_values, X_valid)\n",
        "\n",
        "# force_plotの表示 (最初の観測値を例として)\n",
        "shap.initjs()  # JavaScriptを初期化 (Jupyter Notebookでの表示のため)\n",
        "shap.force_plot(explainer.expected_value[1], shap_values[1][0,:], X_valid.iloc[0,:]) #6m14.7s\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmqPaLfGRVqw"
      },
      "outputs": [],
      "source": [
        "# Set seaborn style\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Plot\n",
        "shap.summary_plot(shap_values, X_valid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odcTuJIWRVqw"
      },
      "source": [
        "# アンサンブル学習"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdqQFw0SRVqw"
      },
      "outputs": [],
      "source": [
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import f1_score, precision_recall_curve\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import time\n",
        "\n",
        "# Specify categorical variables\n",
        "categorical_features = final_data.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# Convert categorical variables to categorical data type\n",
        "for col in categorical_features:\n",
        "    final_data[col] = final_data[col].astype('category')\n",
        "\n",
        "# Split data into training and test sets using the 'is_test' column\n",
        "final_data_train = final_data[final_data['is_test'] == 0]\n",
        "final_data_test = final_data[final_data['is_test'] == 1]\n",
        "\n",
        "# Data split\n",
        "X = final_data_train.drop(columns=['is_fraud?', 'is_test'])\n",
        "y = final_data_train['is_fraud?']\n",
        "\n",
        "# Hyperparameter settings\n",
        "lgb_params = {\n",
        "    'objective': 'binary',\n",
        "    'metric': 'binary_logloss',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 31,\n",
        "    'learning_rate': 0.05,\n",
        "    'feature_fraction': 0.9\n",
        "}\n",
        "\n",
        "xgb_params = {\n",
        "    'objective': 'binary:logistic',\n",
        "    'eval_metric': 'logloss',\n",
        "    'eta': 0.05,\n",
        "    'max_depth': 5\n",
        "}\n",
        "\n",
        "f1_scores = []\n",
        "optimal_thresholds = []\n",
        "\n",
        "folds = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
        "\n",
        "for fold_n, (train_index, valid_index) in tqdm(enumerate(folds.split(X, y))):\n",
        "    print(f'Fold {fold_n + 1} started at {time.ctime()}')\n",
        "    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
        "    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
        "\n",
        "    # LightGBM\n",
        "    train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=categorical_features)\n",
        "    valid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=categorical_features, reference=train_data)\n",
        "    num_round = 10000\n",
        "    bst_lgb = lgb.train(lgb_params, train_data, num_round, valid_sets=[valid_data], callbacks=[lgb.early_stopping(stopping_rounds=500,\n",
        "                                verbose=True), # early_stopping用コールバック関数\n",
        "                           lgb.log_evaluation(1)])\n",
        "\n",
        "    # XGBoost\n",
        "    xgb_train = xgb.DMatrix(X.iloc[train_index], label=y.iloc[train_index], enable_categorical=True)\n",
        "    xgb_valid = xgb.DMatrix(X.iloc[valid_index], label=y.iloc[valid_index], enable_categorical=True)\n",
        "    bst_xgb = xgb.train(xgb_params, xgb_train, num_boost_round=10000, evals=[(xgb_valid, 'eval')], early_stopping_rounds=500, verbose_eval=100)\n",
        "\n",
        "    # Ensemble predictions from LightGBM and XGBoost\n",
        "    y_pred_probs_lgb = bst_lgb.predict(X_valid)\n",
        "    y_pred_probs_xgb = bst_xgb.predict(xgb_valid)\n",
        "    y_pred_probs_avg = (y_pred_probs_lgb + y_pred_probs_xgb) / 2\n",
        "\n",
        "    # Optimal Threshold based on ensemble predictions\n",
        "    precision, recall, thresholds = precision_recall_curve(y_valid, y_pred_probs_avg)\n",
        "    f1_scores_thresholds = 2 * (precision * recall) / (precision + recall)\n",
        "    optimal_threshold = thresholds[np.argmax(f1_scores_thresholds)]\n",
        "    optimal_thresholds.append(optimal_threshold)\n",
        "    y_pred_binary = (y_pred_probs_avg > optimal_threshold).astype(int)\n",
        "\n",
        "    f1 = f1_score(y_valid, y_pred_binary)\n",
        "    f1_scores.append(f1)\n",
        "\n",
        "    # Save models\n",
        "    bst_lgb.save_model(f'D:\\\\MUFG Data Science Champion Ship 2023\\\\model\\\\lgb_model_fold{fold_n + 1}.txt')\n",
        "    bst_xgb.save_model(f'D:\\\\MUFG Data Science Champion Ship 2023\\\\model\\\\xgb_model_fold{fold_n + 1}.txt')\n",
        "\n",
        "print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(f1_scores), np.std(f1_scores)))\n",
        "#24m13.8s\n",
        "\n",
        "#after Drop\n",
        "#17m21.0s\n",
        "#CV mean score: 0.6444, std: 0.0045.\n",
        "\n",
        "#after Dorp2\n",
        "# index, user_id, address\n",
        "#24m35.2s\n",
        "#CV mean score: 0.6442, std: 0.0038.\n",
        "\n",
        "#500\n",
        "#35m27.3s\n",
        "#CV mean score: 0.6472, std: 0.0014\n",
        "\n",
        "#1000\n",
        "#38m\n",
        "#CV mean score: 0.6472, std: 0.0014\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvhqhmYvRVqx"
      },
      "source": [
        "## Separate models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwGBg_NnRVqx"
      },
      "outputs": [],
      "source": [
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import f1_score, precision_recall_curve\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "def train_model_for_transaction_type(data, transaction_type):\n",
        "    # Filter the data based on the specified transaction type\n",
        "    filtered_data = data[data['use_chip'] == transaction_type]\n",
        "\n",
        "    # Specify categorical variables\n",
        "    categorical_features = filtered_data.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "    # 'use_chip' column is not needed in the categorical features list as it's already filtered out\n",
        "    if 'use_chip' in categorical_features:\n",
        "        categorical_features.remove('use_chip')\n",
        "\n",
        "    # Convert categorical variables to categorical data type\n",
        "    for col in categorical_features:\n",
        "        filtered_data[col] = filtered_data[col].astype('category')\n",
        "\n",
        "    # Split data using 'is_test' column\n",
        "    filtered_data_train = filtered_data[filtered_data['is_test'] == 0]\n",
        "    filtered_data_test = filtered_data[filtered_data['is_test'] == 1]\n",
        "\n",
        "    # Data split\n",
        "    X = filtered_data_train.drop(columns=['is_fraud?', 'is_test', 'use_chip'])\n",
        "    y = filtered_data_train['is_fraud?']\n",
        "\n",
        "    # Hyperparameter settings\n",
        "    lgb_params = {\n",
        "        'objective': 'binary',\n",
        "        'metric': 'binary_logloss',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'num_leaves': 31,\n",
        "        'learning_rate': 0.05,\n",
        "        'feature_fraction': 0.9\n",
        "    }\n",
        "\n",
        "    xgb_params = {\n",
        "        'objective': 'binary:logistic',\n",
        "        'eval_metric': 'logloss',\n",
        "        'eta': 0.05,\n",
        "        'max_depth': 5\n",
        "    }\n",
        "\n",
        "    f1_scores = []\n",
        "    optimal_thresholds = []\n",
        "\n",
        "    folds = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
        "\n",
        "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n",
        "        print(f'{transaction_type}Fold {fold_n + 1} started at {time.ctime()}')\n",
        "        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
        "        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
        "\n",
        "        # LightGBM\n",
        "        train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=categorical_features)\n",
        "        valid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=categorical_features, reference=train_data)\n",
        "        num_round = 10000\n",
        "        bst_lgb = lgb.train(lgb_params, train_data, num_round, valid_sets=[valid_data], callbacks=[lgb.early_stopping(stopping_rounds=500, verbose=True)])\n",
        "\n",
        "        # XGBoost\n",
        "        xgb_train = xgb.DMatrix(X.iloc[train_index], label=y.iloc[train_index], enable_categorical=True)\n",
        "        xgb_valid = xgb.DMatrix(X.iloc[valid_index], label=y.iloc[valid_index], enable_categorical=True)\n",
        "        bst_xgb = xgb.train(xgb_params, xgb_train, num_boost_round=10000, evals=[(xgb_valid, 'eval')], early_stopping_rounds=500, verbose_eval=100)\n",
        "\n",
        "        # Ensemble predictions from LightGBM and XGBoost\n",
        "        y_pred_probs_lgb = bst_lgb.predict(X_valid)\n",
        "        y_pred_probs_xgb = bst_xgb.predict(xgb_valid)\n",
        "        y_pred_probs_avg = (y_pred_probs_lgb + y_pred_probs_xgb) / 2\n",
        "\n",
        "        # Optimal Threshold based on ensemble predictions\n",
        "        precision, recall, thresholds = precision_recall_curve(y_valid, y_pred_probs_avg)\n",
        "        f1_scores_thresholds = 2 * (precision * recall) / (precision + recall)\n",
        "        optimal_threshold = thresholds[np.argmax(f1_scores_thresholds)]\n",
        "        optimal_thresholds.append(optimal_threshold)\n",
        "        y_pred_binary = (y_pred_probs_avg > optimal_threshold).astype(int)\n",
        "\n",
        "        f1 = f1_score(y_valid, y_pred_binary)\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "\n",
        "        #Save models\n",
        "        bst_lgb.save_model(f'D:\\\\MUFG Data Science Champion Ship 2023\\\\model\\\\separate models\\\\lgb_model_fold{fold_n + 1}.txt')\n",
        "        bst_xgb.save_model(f'D:\\\\MUFG Data Science Champion Ship 2023\\\\model\\\\separate models\\\\xgb_model_fold{fold_n + 1}.txt')\n",
        "\n",
        "    print(f'{transaction_type} - CV mean score: {np.mean(f1_scores):.4f}, std: {np.std(f1_scores):.4f}.')\n",
        "\n",
        "# Train models for each transaction type\n",
        "transaction_types = ['Swipe Transaction', 'Chip Transaction', 'Online Transaction']\n",
        "for transaction_type in transaction_types:\n",
        "    train_model_for_transaction_type(final_data, transaction_type)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Q2Srs-cRVqx"
      },
      "source": [
        "### Feature importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5bPvYInRVqx"
      },
      "outputs": [],
      "source": [
        "# Function to map XGBoost feature names to original dataset column names\n",
        "def map_xgb_feature_names(xgb_importance, original_features):\n",
        "    mapped_importance = {}\n",
        "    for key, value in xgb_importance.items():\n",
        "        # Extract the feature index from the XGBoost feature name (like 'f0', 'f1', etc.)\n",
        "        feature_idx = int(key[1:])\n",
        "        mapped_importance[original_features[feature_idx]] = value\n",
        "    return mapped_importance\n",
        "\n",
        "# Updated function to plot feature importance using seaborn with XGBoost feature name correction\n",
        "def plot_feature_importance_seaborn_final(lgb_model, xgb_model, transaction_type):\n",
        "    # LightGBM feature importance\n",
        "    lgb_importance = lgb_model.feature_importance(importance_type='gain')\n",
        "    lgb_features = lgb_model.feature_name()\n",
        "    lgb_sorted_idx = np.argsort(lgb_importance)[::-1]\n",
        "\n",
        "    # XGBoost feature importance\n",
        "    xgb_importance = xgb_model.get_score(importance_type='gain')\n",
        "\n",
        "    # If XGBoost model does not have feature names, map them\n",
        "    if not xgb_model.feature_names:\n",
        "        xgb_importance = map_xgb_feature_names(xgb_importance, X.columns)\n",
        "    xgb_sorted_idx = sorted(xgb_importance, key=xgb_importance.get, reverse=True)\n",
        "\n",
        "    # Create a DataFrame for seaborn plotting\n",
        "    lgb_df = pd.DataFrame({\n",
        "        'Feature': np.array(lgb_features)[lgb_sorted_idx][:10],\n",
        "        'Importance': lgb_importance[lgb_sorted_idx][:10]\n",
        "    })\n",
        "    xgb_df = pd.DataFrame({\n",
        "        'Feature': xgb_sorted_idx[:10],\n",
        "        'Importance': [xgb_importance[i] for i in xgb_sorted_idx][:10]\n",
        "    })\n",
        "\n",
        "    # Plot feature importance using seaborn\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(14, 10))\n",
        "\n",
        "    # LightGBM\n",
        "    sns.barplot(data=lgb_df, y='Feature', x='Importance', ax=ax[0], palette=\"viridis\")\n",
        "    ax[0].set_title(f'LightGBM - Top 10 Feature Importance for {transaction_type}')\n",
        "    ax[0].set_xlabel('Importance')\n",
        "    ax[0].set_ylabel('Features')\n",
        "\n",
        "    # XGBoost\n",
        "    sns.barplot(data=xgb_df, y='Feature', x='Importance', ax=ax[1], palette=\"viridis\")\n",
        "    ax[1].set_title(f'XGBoost - Top 10 Feature Importance for {transaction_type}')\n",
        "    ax[1].set_xlabel('Importance')\n",
        "    ax[1].set_ylabel('Features')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Load the saved models and plot feature importance for each transaction type\n",
        "lgb_online_model = lgb.Booster(model_file='D:\\\\MUFG Data Science Champion Ship 2023\\\\model\\\\separate models\\\\lgb_model_Online Transaction_fold3.txt')\n",
        "xgb_online_model = xgb.Booster(model_file='D:\\\\MUFG Data Science Champion Ship 2023\\\\model\\\\separate models\\\\xgb_model_Online Transaction_fold3.txt')\n",
        "\n",
        "lgb_other_model = lgb.Booster(model_file='D:\\\\MUFG Data Science Champion Ship 2023\\\\model\\\\separate models\\\\lgb_model_Other_fold3.txt')\n",
        "xgb_other_model = xgb.Booster(model_file='D:\\\\MUFG Data Science Champion Ship 2023\\\\model\\\\separate models\\\\xgb_model_Other_fold3.txt')\n",
        "\n",
        "plot_feature_importance_seaborn_final(lgb_online_model, xgb_online_model, 'Online Transaction')\n",
        "plot_feature_importance_seaborn_final(lgb_other_model, xgb_other_model, 'Other')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "af_pLoSsRVqy"
      },
      "outputs": [],
      "source": [
        "# Updated function to compute and plot SHAP values for LightGBM and XGBoost models,\n",
        "# while accepting models directly as arguments and without needing the dataset explicitly\n",
        "def plot_shap_values_final_updated(lgb_model, xgb_model, transaction_type):\n",
        "\n",
        "    # To get SHAP values, we still need some data. We'll use a sample from the model's training data.\n",
        "    # This won't give exact insights for all data but is a common approach for global interpretability.\n",
        "    lgb_data_sample = lgb.Dataset.get_data(lgb_model.train_set).sample(1000, random_state=42)\n",
        "    xgb_data_sample = xgb.DMatrix(lgb_data_sample, feature_names=lgb_model.feature_name())\n",
        "\n",
        "    # LightGBM SHAP values\n",
        "    explainer_lgb = shap.Explainer(lgb_model)\n",
        "    shap_values_lgb = explainer_lgb(lgb_data_sample)\n",
        "\n",
        "    # XGBoost SHAP values\n",
        "    explainer_xgb = shap.Explainer(xgb_model)\n",
        "    shap_values_xgb = explainer_xgb(xgb_data_sample)\n",
        "\n",
        "    # Plot\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # LightGBM\n",
        "    shap.summary_plot(shap_values_lgb, lgb_data_sample, plot_type=\"bar\", show=False, ax=ax[0])\n",
        "    ax[0].set_title(f'LightGBM SHAP Values for {transaction_type}')\n",
        "\n",
        "    # XGBoost\n",
        "    shap.summary_plot(shap_values_xgb, lgb_data_sample, plot_type=\"bar\", show=False, ax=ax[1])\n",
        "    ax[1].set_title(f'XGBoost SHAP Values for {transaction_type}')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Note: The below code will not work in this environment due to the mentioned reasons\n",
        "plot_shap_values_final_updated(lgb_online_model, xgb_online_model, 'Online Transaction')\n",
        "plot_shap_values_final_updated(lgb_other_model, xgb_other_model, 'Other')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9e7n_SmRVqy"
      },
      "source": [
        "## F1スコアで最適化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_L-HByN9RVqy"
      },
      "outputs": [],
      "source": [
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import f1_score, precision_recall_curve\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "# ... [データの準備や前処理] ...\n",
        "# Specify categorical variables\n",
        "categorical_features = final_data.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# Convert categorical variables to categorical data type\n",
        "for col in categorical_features:\n",
        "    final_data[col] = final_data[col].astype('category')\n",
        "\n",
        "# Split data into training and test sets using the 'is_test' column\n",
        "final_data_train = final_data[final_data['is_test'] == 0]\n",
        "final_data_test = final_data[final_data['is_test'] == 1]\n",
        "\n",
        "# Data split\n",
        "X = final_data_train.drop(columns=['is_fraud?', 'is_test'])\n",
        "y = final_data_train['is_fraud?']\n",
        "\n",
        "# Hyperparameter settings\n",
        "lgb_params = {\n",
        "    'objective': 'binary',\n",
        "    'metric': 'binary_logloss',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 31,\n",
        "    'learning_rate': 0.05,\n",
        "    'feature_fraction': 0.9\n",
        "}\n",
        "\n",
        "xgb_params = {\n",
        "    'objective': 'binary:logistic',\n",
        "    'eval_metric': 'logloss',\n",
        "    'eta': 0.05,\n",
        "    'max_depth': 5\n",
        "}\n",
        "\n",
        "\n",
        "# F1スコアの評価関数 (LightGBM用)\n",
        "def lgb_f1_score(y_pred, data):\n",
        "    y_true = data.get_label()\n",
        "    precision, recall, thresholds = precision_recall_curve(y_true, y_pred)\n",
        "    f1_scores_thresholds = 2 * (precision * recall) / (precision + recall)\n",
        "    best_threshold = thresholds[np.argmax(f1_scores_thresholds)]\n",
        "    y_pred_binary = (y_pred > best_threshold).astype(int)\n",
        "    return 'f1', f1_score(y_true, y_pred_binary), True\n",
        "\n",
        "# F1スコアの評価関数 (XGBoost用)\n",
        "def xgb_f1_score(y_pred, data):\n",
        "    y_true = data.get_label()\n",
        "    precision, recall, thresholds = precision_recall_curve(y_true, y_pred)\n",
        "    f1_scores_thresholds = 2 * (precision * recall) / (precision + recall)\n",
        "    best_threshold = thresholds[np.argmax(f1_scores_thresholds)]\n",
        "    y_pred_binary = (y_pred > best_threshold).astype(int)\n",
        "    return 'f1', f1_score(y_true, y_pred_binary)\n",
        "\n",
        "f1_scores = []\n",
        "optimal_thresholds = []\n",
        "\n",
        "folds = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
        "\n",
        "for fold_n, (train_index, valid_index) in tqdm(enumerate(folds.split(X, y))):\n",
        "    print(f'Fold {fold_n + 1} started at {time.ctime()}')\n",
        "    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
        "    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
        "\n",
        "    # LightGBM\n",
        "    train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=categorical_features)\n",
        "    valid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=categorical_features, reference=train_data)\n",
        "    num_round = 10000\n",
        "    bst_lgb = lgb.train(lgb_params,\n",
        "                        train_data,\n",
        "                        num_round,\n",
        "                        valid_sets=[valid_data],\n",
        "                        feval=lgb_f1_score,  # ここでカスタム評価関数を指定\n",
        "                        callbacks=[lgb.early_stopping(stopping_rounds=1000,\n",
        "                                verbose=True), # early_stopping用コールバック関数\n",
        "                           lgb.log_evaluation(1)]\n",
        "                        )\n",
        "\n",
        "    # XGBoost\n",
        "    xgb_train = xgb.DMatrix(X.iloc[train_index], label=y.iloc[train_index], enable_categorical=True)\n",
        "    xgb_valid = xgb.DMatrix(X.iloc[valid_index], label=y.iloc[valid_index], enable_categorical=True)\n",
        "    bst_xgb = xgb.train(xgb_params,\n",
        "                        xgb_train,\n",
        "                        num_boost_round=10000,\n",
        "                        evals=[(xgb_valid, 'eval')],\n",
        "                        feval=xgb_f1_score,  # ここでカスタム評価関数を指定\n",
        "                        early_stopping_rounds=1000,\n",
        "                        verbose_eval=100)\n",
        "\n",
        "    # Ensemble predictions\n",
        "    y_pred_probs_lgb = bst_lgb.predict(X_valid)\n",
        "    y_pred_probs_xgb = bst_xgb.predict(xgb_valid)\n",
        "    y_pred_probs_avg = (y_pred_probs_lgb + y_pred_probs_xgb) / 2\n",
        "\n",
        "    # Optimal Threshold based on ensemble predictions\n",
        "    precision, recall, thresholds = precision_recall_curve(y_valid, y_pred_probs_avg)\n",
        "    f1_scores_thresholds = 2 * (precision * recall) / (precision + recall)\n",
        "    optimal_threshold = thresholds[np.argmax(f1_scores_thresholds)]\n",
        "    optimal_thresholds.append(optimal_threshold)\n",
        "    y_pred_binary = (y_pred_probs_avg > optimal_threshold).astype(int)\n",
        "\n",
        "    f1 = f1_score(y_valid, y_pred_binary)\n",
        "    f1_scores.append(f1)\n",
        "\n",
        "    # Save models\n",
        "    bst_lgb.save_model(f'D:\\\\MUFG Data Science Champion Ship 2023\\\\model\\\\lgb_model_fold{fold_n + 1}.txt')\n",
        "    bst_xgb.save_model(f'D:\\\\MUFG Data Science Champion Ship 2023\\\\model\\\\xgb_model_fold{fold_n + 1}.txt')\n",
        "\n",
        "print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(f1_scores), np.std(f1_scores)))\n",
        "\n",
        "#50\n",
        "#3m56.6\n",
        "#CV mean score: 0.6204, std: 0.0026.\n",
        "\n",
        "#100\n",
        "#11m34.5s\n",
        "#CV mean score: 0.6245, std: 0.0018.\n",
        "\n",
        "#200\n",
        "#15m16.5s\n",
        "#CV mean score: 0.6300, std: 0.0029.\n",
        "\n",
        "#500\n",
        "#20m4.4s\n",
        "#CV mean score: 0.6344, std: 0.0033.\n",
        "\n",
        "#1000\n",
        "#27m.33.0s\n",
        "#CV mean score: 0.6387, std: 0.0025.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7iUuP07RVqz"
      },
      "source": [
        "### F1score optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPBfdXXERVqz"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "\n",
        "def objective(trial):\n",
        "    # Hyperparameters to be optimized with Optuna\n",
        "    lgb_params = {\n",
        "        'objective': 'binary',\n",
        "        'metric': 'binary_logloss',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 10, 500),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1),\n",
        "        'feature_fraction': trial.suggest_float('feature_fraction', 0.1, 1.0)\n",
        "    }\n",
        "\n",
        "    xgb_params = {\n",
        "        'objective': 'binary:logistic',\n",
        "        'eval_metric': 'logloss',\n",
        "        'eta': trial.suggest_float('eta', 0.001, 0.1),\n",
        "        'max_depth': trial.suggest_int('max_depth', 1, 10)\n",
        "    }\n",
        "\n",
        "    f1_scores = []\n",
        "\n",
        "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n",
        "        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
        "        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
        "\n",
        "        # LightGBM\n",
        "        train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=categorical_features)\n",
        "        valid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=categorical_features, reference=train_data)\n",
        "        num_round = 10000\n",
        "        bst_lgb = lgb.train(lgb_params, train_data, num_round, valid_sets=[valid_data], feval=lgb_f1_score, callbacks=[lgb.early_stopping(stopping_rounds=1000, verbose=True), lgb.log_evaluation(1)])\n",
        "\n",
        "        # XGBoost\n",
        "        xgb_train = xgb.DMatrix(X.iloc[train_index], label=y.iloc[train_index], enable_categorical=True)\n",
        "        xgb_valid = xgb.DMatrix(X.iloc[valid_index], label=y.iloc[valid_index], enable_categorical=True)\n",
        "        bst_xgb = xgb.train(xgb_params, xgb_train, num_boost_round=10000, evals=[(xgb_valid, 'eval')], feval=xgb_f1_score, early_stopping_rounds=1000, verbose_eval=100)\n",
        "\n",
        "        # Ensemble predictions\n",
        "        y_pred_probs_lgb = bst_lgb.predict(X_valid)\n",
        "        y_pred_probs_xgb = bst_xgb.predict(xgb_valid)\n",
        "        y_pred_probs_avg = (y_pred_probs_lgb + y_pred_probs_xgb) / 2\n",
        "\n",
        "        # Optimal Threshold\n",
        "        precision, recall, thresholds = precision_recall_curve(y_valid, y_pred_probs_avg)\n",
        "        f1_scores_thresholds = 2 * (precision * recall) / (precision + recall)\n",
        "        optimal_threshold = thresholds[np.argmax(f1_scores_thresholds)]\n",
        "        y_pred_binary = (y_pred_probs_avg > optimal_threshold).astype(int)\n",
        "\n",
        "        f1 = f1_score(y_valid, y_pred_binary)\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "    return -np.mean(f1_scores)  # We use negative because Optuna tries to minimize the objective\n",
        "\n",
        "# Create study object\n",
        "study = optuna.create_study()\n",
        "study.optimize(objective, n_trials=100)  # Adjust the number of trials\n",
        "\n",
        "print(\"Number of finished trials: \", len(study.trials))\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "print(\"  Value: \", trial.value)\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(f\"    {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fr5h0IoiRVqz"
      },
      "source": [
        "## Optuna適用"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5G-2ud4RVqz"
      },
      "outputs": [],
      "source": [
        "# import lightgbm as lgb\n",
        "# import xgboost as xgb\n",
        "# from sklearn.metrics import f1_score, precision_recall_curve\n",
        "# from tqdm import tqdm\n",
        "# from sklearn.model_selection import StratifiedKFold\n",
        "# import time\n",
        "\n",
        "# # Specify categorical variables\n",
        "# categorical_features = final_data.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# # Convert categorical variables to categorical data type\n",
        "# for col in categorical_features:\n",
        "#     final_data[col] = final_data[col].astype('category')\n",
        "\n",
        "# # Split data into training and test sets using the 'is_test' column\n",
        "# final_data_train = final_data[final_data['is_test'] == 0]\n",
        "# final_data_test = final_data[final_data['is_test'] == 1]\n",
        "\n",
        "# # Data split\n",
        "# X = final_data_train.drop(columns=['is_fraud?', 'is_test'])\n",
        "# y = final_data_train['is_fraud?']\n",
        "\n",
        "# # Hyperparameter settings\n",
        "# lgb_params = {\n",
        "#     'objective': 'binary',\n",
        "#     'metric': 'binary_logloss',\n",
        "#     'boosting_type': 'gbdt',\n",
        "#     'num_leaves': 31,\n",
        "#     'learning_rate': 0.05,\n",
        "#     'feature_fraction': 0.9\n",
        "# }\n",
        "\n",
        "# xgb_params = {\n",
        "#     'objective': 'binary:logistic',\n",
        "#     'eval_metric': 'logloss',\n",
        "#     'eta': 0.05,\n",
        "#     'max_depth': 5\n",
        "# }\n",
        "\n",
        "# f1_scores = []\n",
        "# optimal_thresholds = []\n",
        "\n",
        "# folds = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
        "\n",
        "# def objective(trial):\n",
        "#     # LightGBM hyperparameters\n",
        "#     lgb_params = {\n",
        "#         'objective': 'binary',\n",
        "#         'metric': 'binary_logloss',\n",
        "#         'boosting_type': 'gbdt',\n",
        "#         'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
        "#         'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-1, log=True),\n",
        "#         'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
        "#     }\n",
        "\n",
        "#     # XGBoost hyperparameters\n",
        "#     xgb_params = {\n",
        "#         'objective': 'binary:logistic',\n",
        "#         'eval_metric': 'logloss',\n",
        "#         'eta': trial.suggest_float('eta', 1e-4, 1e-1, log=True),\n",
        "#         'max_depth': trial.suggest_int('max_depth', 1, 9),\n",
        "#     }\n",
        "\n",
        "#     for fold_n, (train_index, valid_index) in tqdm(enumerate(folds.split(X, y))):\n",
        "#         print(f'Fold {fold_n + 1} started at {time.ctime()}')\n",
        "#         X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
        "#         y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
        "\n",
        "#         # LightGBM\n",
        "#         train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=categorical_features)\n",
        "#         valid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=categorical_features, reference=train_data)\n",
        "#         num_round = 1000\n",
        "#         bst_lgb = lgb.train(lgb_params, train_data, num_round, valid_sets=[valid_data])\n",
        "\n",
        "#         # XGBoost\n",
        "#         xgb_train = xgb.DMatrix(X.iloc[train_index], label=y.iloc[train_index], enable_categorical=True)\n",
        "#         xgb_valid = xgb.DMatrix(X.iloc[valid_index], label=y.iloc[valid_index], enable_categorical=True)\n",
        "#         bst_xgb = xgb.train(xgb_params, xgb_train, num_boost_round=1000, evals=[(xgb_valid, 'eval')])\n",
        "\n",
        "#         # Ensemble predictions from LightGBM and XGBoost\n",
        "#         y_pred_probs_lgb = bst_lgb.predict(X_valid)\n",
        "#         y_pred_probs_xgb = bst_xgb.predict(xgb_valid)\n",
        "#         y_pred_probs_avg = (y_pred_probs_lgb + y_pred_probs_xgb) / 2\n",
        "\n",
        "#         # Optimal Threshold based on ensemble predictions\n",
        "#         precision, recall, thresholds = precision_recall_curve(y_valid, y_pred_probs_avg)\n",
        "#         f1_scores_thresholds = 2 * (precision * recall) / (precision + recall)\n",
        "#         optimal_threshold = thresholds[np.argmax(f1_scores_thresholds)]\n",
        "#         optimal_thresholds.append(optimal_threshold)\n",
        "#         y_pred_binary = (y_pred_probs_avg > optimal_threshold).astype(int)\n",
        "\n",
        "#         f1 = f1_score(y_valid, y_pred_binary)\n",
        "#         f1_scores.append(f1)\n",
        "\n",
        "#         # Save models\n",
        "#         bst_lgb.save_model(f'D:\\\\MUFG Data Science Champion Ship 2023\\\\model\\\\lgb_model_fold{fold_n + 1}.txt')\n",
        "#         bst_xgb.save_model(f'D:\\\\MUFG Data Science Champion Ship 2023\\\\model\\\\xgb_model_fold{fold_n + 1}.txt')\n",
        "\n",
        "#         return np.mean(f1_scores)\n",
        "\n",
        "# study = optuna.create_study(direction='maximize')\n",
        "# study.optimize(objective, n_trials=100)\n",
        "\n",
        "# # 最適なハイパーパラメータを表示\n",
        "# print('Number of finished trials:', len(study.trials))\n",
        "# print('Best trial:')\n",
        "# trial = study.best_trial\n",
        "\n",
        "# print('  Value: {:.4f}'.format(trial.value))\n",
        "# print('  Params: ')\n",
        "# for key, value in trial.params.items():\n",
        "#     print('    {}: {}'.format(key, value))\n",
        "\n",
        "# print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(f1_scores), np.std(f1_scores))) #174m4.9s kakatta\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VP4FqL_wRVqz"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "\n",
        "def objective(trial):\n",
        "    # Hyperparameters to be optimized with Optuna\n",
        "    lgb_params = {\n",
        "        'objective': 'binary',\n",
        "        'metric': 'binary_logloss',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 10, 500),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1),\n",
        "        'feature_fraction': trial.suggest_float('feature_fraction', 0.1, 1.0)\n",
        "    }\n",
        "\n",
        "    xgb_params = {\n",
        "        'objective': 'binary:logistic',\n",
        "        'eval_metric': 'logloss',\n",
        "        'eta': trial.suggest_float('eta', 0.001, 0.1),\n",
        "        'max_depth': trial.suggest_int('max_depth', 1, 10)\n",
        "    }\n",
        "\n",
        "    f1_scores = []\n",
        "\n",
        "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n",
        "        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
        "        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
        "\n",
        "        # LightGBM\n",
        "        train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=categorical_features)\n",
        "        valid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=categorical_features, reference=train_data)\n",
        "        num_round = 10000\n",
        "        bst_lgb = lgb.train(lgb_params, train_data, num_round, valid_sets=[valid_data], callbacks=[lgb.early_stopping(stopping_rounds=500, verbose=True),\n",
        "                                                                                                     lgb.log_evaluation(1)])\n",
        "\n",
        "        # XGBoost\n",
        "        xgb_train = xgb.DMatrix(X.iloc[train_index], label=y.iloc[train_index], enable_categorical=True)\n",
        "        xgb_valid = xgb.DMatrix(X.iloc[valid_index], label=y.iloc[valid_index], enable_categorical=True)\n",
        "        bst_xgb = xgb.train(xgb_params, xgb_train, num_boost_round=10000, evals=[(xgb_valid, 'eval')], early_stopping_rounds=500, verbose_eval=100)\n",
        "\n",
        "        # Ensemble predictions from LightGBM and XGBoost\n",
        "        y_pred_probs_lgb = bst_lgb.predict(X_valid)\n",
        "        y_pred_probs_xgb = bst_xgb.predict(xgb_valid)\n",
        "        y_pred_probs_avg = (y_pred_probs_lgb + y_pred_probs_xgb) / 2\n",
        "\n",
        "        # Optimal Threshold based on ensemble predictions\n",
        "        precision, recall, thresholds = precision_recall_curve(y_valid, y_pred_probs_avg)\n",
        "        f1_scores_thresholds = 2 * (precision * recall) / (precision + recall)\n",
        "        optimal_threshold = thresholds[np.argmax(f1_scores_thresholds)]\n",
        "        y_pred_binary = (y_pred_probs_avg > optimal_threshold).astype(int)\n",
        "\n",
        "        f1 = f1_score(y_valid, y_pred_binary)\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "    return -np.mean(f1_scores)\n",
        "\n",
        "study = optuna.create_study()\n",
        "study.optimize(objective, n_trials=200)  # You can adjust the number of trials\n",
        "\n",
        "print(\"Number of finished trials: \", len(study.trials))\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "print(\"  Value: \", trial.value)\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(f\"    {key}: {value}\")\n",
        "\n",
        "print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(f1_scores), np.std(f1_scores)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsJvarUnRVq0"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xHteVK4RVq9"
      },
      "outputs": [],
      "source": [
        "# Test data predictions\n",
        "predictions_lgb = []\n",
        "predictions_xgb = []\n",
        "\n",
        "# Get features for the test data\n",
        "final_data_test_features = final_data_test.drop(columns=['is_fraud?', 'is_test'])\n",
        "xgb_test = xgb.DMatrix(final_data_test_features, enable_categorical=True)\n",
        "\n",
        "# Get predictions for each model\n",
        "for fold_n in range(4):\n",
        "    # LightGBM\n",
        "    bst_lgb = lgb.Booster(model_file=f'D:\\\\MUFG Data Science Champion Ship 2023\\\\model\\\\lgb_model_fold{fold_n + 1}.txt')\n",
        "    preds_lgb = bst_lgb.predict(final_data_test_features, num_iteration=bst_lgb.best_iteration)\n",
        "    predictions_lgb.append(preds_lgb)\n",
        "\n",
        "    # XGBoost\n",
        "    bst_xgb = xgb.Booster(model_file=f'D:\\\\MUFG Data Science Champion Ship 2023\\\\model\\\\xgb_model_fold{fold_n + 1}.txt')\n",
        "    preds_xgb = bst_xgb.predict(xgb_test)\n",
        "    predictions_xgb.append(preds_xgb)\n",
        "\n",
        "# Get the average predictions\n",
        "mean_preds_lgb = np.mean(predictions_lgb, axis=0)\n",
        "mean_preds_xgb = np.mean(predictions_xgb, axis=0)\n",
        "\n",
        "# Average predictions from the two models\n",
        "mean_preds_ensemble = (mean_preds_lgb + mean_preds_xgb) / 2\n",
        "\n",
        "# Get the average of the optimal thresholds\n",
        "mean_optimal_threshold = np.mean(optimal_thresholds)\n",
        "\n",
        "# Convert the predictions to binary\n",
        "binary_predictions = (mean_preds_ensemble > mean_optimal_threshold).astype(int)\n",
        "\n",
        "# Create a dataframe for submission\n",
        "submission_df = pd.DataFrame({\n",
        "    'ID': df_test['index'].values,\n",
        "    'is_fraud?': binary_predictions\n",
        "})\n",
        "\n",
        "# Save to CSV without header and index\n",
        "submission_df.to_csv('D:\\\\MUFG Data Science Champion Ship 2023\\\\predictions\\\\submit10.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "au1Nz0etRVq9"
      },
      "outputs": [],
      "source": [
        "# Optunaで最適化されたハイパーパラメータを取得\n",
        "best_params_lgb = {\n",
        "    'objective': 'binary',\n",
        "    'metric': 'binary_logloss',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': study.best_params['num_leaves'],\n",
        "    'learning_rate': study.best_params['learning_rate'],\n",
        "    'feature_fraction': study.best_params['feature_fraction']\n",
        "}\n",
        "\n",
        "best_params_xgb = {\n",
        "    'objective': 'binary:logistic',\n",
        "    'eval_metric': 'logloss',\n",
        "    'eta': study.best_params['eta'],\n",
        "    'max_depth': study.best_params['max_depth']\n",
        "}\n",
        "\n",
        "# テストデータの予測用の空のリストを初期化\n",
        "predictions_lgb = []\n",
        "predictions_xgb = []\n",
        "\n",
        "\n",
        "# テストデータの特徴量を取得\n",
        "final_data_test_features = final_data_test.drop(columns=['is_fraud?', 'is_test'])\n",
        "xgb_test = xgb.DMatrix(final_data_test_features, enable_categorical=True)\n",
        "\n",
        "# 各モデルに対して予測を行う\n",
        "for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n",
        "    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
        "    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
        "\n",
        "    # LightGBM\n",
        "    train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=categorical_features)\n",
        "    valid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=categorical_features, reference=train_data)\n",
        "    num_round = 10000\n",
        "    bst_lgb = lgb.train(best_params_lgb, train_data, num_round, valid_sets=[valid_data], callbacks=[lgb.early_stopping(stopping_rounds=500, verbose=True)])\n",
        "    preds_lgb = bst_lgb.predict(final_data_test_features)\n",
        "    predictions_lgb.append(preds_lgb)\n",
        "\n",
        "    # XGBoost\n",
        "    xgb_train = xgb.DMatrix(X.iloc[train_index], label=y.iloc[train_index], enable_categorical=True)\n",
        "    bst_xgb = xgb.train(best_params_xgb, xgb_train, num_boost_round=10000, evals=[(xgb_valid, 'eval')], early_stopping_rounds=500)\n",
        "    preds_xgb = bst_xgb.predict(xgb_test)\n",
        "    predictions_xgb.append(preds_xgb)\n",
        "\n",
        "# 予測の平均を取得\n",
        "mean_preds_lgb = np.mean(predictions_lgb, axis=0)\n",
        "mean_preds_xgb = np.mean(predictions_xgb, axis=0)\n",
        "mean_preds_ensemble = (mean_preds_lgb + mean_preds_xgb) / 2\n",
        "\n",
        "# 平均の最適な閾値を取得\n",
        "mean_optimal_threshold = np.mean(optimal_thresholds)\n",
        "\n",
        "# 予測を二値に変換\n",
        "binary_predictions = (mean_preds_ensemble > mean_optimal_threshold).astype(int)\n",
        "\n",
        "# 送信用のデータフレームを作成\n",
        "submission_df = pd.DataFrame({\n",
        "    'ID': df_test['index'].values,\n",
        "    'is_fraud?': binary_predictions\n",
        "})\n",
        "\n",
        "# CSVに保存（ヘッダーとインデックスなし）\n",
        "submission_df.to_csv('D:\\\\MUFG Data Science Champion Ship 2023\\\\predictions\\\\submit0912v2.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jISMT-YVRVq-"
      },
      "outputs": [],
      "source": [
        "# すべてのカテゴリ変数のカラムについて、学習データと検証データの一意の値を比較します。\n",
        "for col in categorical_features:\n",
        "    train_unique_values = set(X_train[col].unique())\n",
        "    valid_unique_values = set(X_valid[col].unique())\n",
        "\n",
        "    # 学習データにのみ存在するカテゴリを検出\n",
        "    only_train_values = train_unique_values - valid_unique_values\n",
        "    if only_train_values:\n",
        "        print(f\"Only in training data for {col}: {only_train_values}\")\n",
        "\n",
        "    # 検証データにのみ存在するカテゴリを検出\n",
        "    only_valid_values = valid_unique_values - train_unique_values\n",
        "    if only_valid_values:\n",
        "        print(f\"Only in validation data for {col}: {only_valid_values}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAjiJmW1RVq-"
      },
      "outputs": [],
      "source": [
        "if 'merchant_city' in final_data_test_features.columns:\n",
        "    print(\"'merchant_city' exists in final_data_test_features.\")\n",
        "else:\n",
        "    print(\"'merchant_city' does not exist in final_data_test_features.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-B7yyDBRVq-"
      },
      "source": [
        "## Separate models test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "II7lzQOlRVq-"
      },
      "outputs": [],
      "source": [
        "def get_predictions_for_transaction_type(data, df_test, transaction_type):\n",
        "    # Filter test data for the given transaction type\n",
        "    filtered_data_test = data[(data['use_chip'] == transaction_type) & (data['is_test'] == 1)]\n",
        "\n",
        "    # Specify categorical variables\n",
        "    categorical_features = filtered_data_test.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "    # 'use_chip' column is not needed in the categorical features list as it's already filtered out\n",
        "    if 'use_chip' in categorical_features:\n",
        "        categorical_features.remove('use_chip')\n",
        "\n",
        "    # Convert categorical variables to categorical data type\n",
        "    for col in categorical_features:\n",
        "        filtered_data_test[col] = filtered_data_test[col].astype('category')\n",
        "\n",
        "    # Get features for the filtered test data\n",
        "    filtered_data_test_features = filtered_data_test.drop(columns=['is_fraud?', 'is_test', 'use_chip'])\n",
        "    xgb_test = xgb.DMatrix(filtered_data_test_features, enable_categorical=True)\n",
        "\n",
        "    predictions_lgb = []\n",
        "    predictions_xgb = []\n",
        "\n",
        "    # Get predictions for each model\n",
        "    for fold_n in range(4):\n",
        "        # LightGBM\n",
        "        bst_lgb = lgb.Booster(model_file=f'D:\\\\MUFG Data Science Champion Ship 2023\\\\model\\\\separate models\\\\lgb_model_{transaction_type}_fold{fold_n + 1}.txt')\n",
        "        preds_lgb = bst_lgb.predict(filtered_data_test_features, num_iteration=bst_lgb.best_iteration)\n",
        "        predictions_lgb.append(preds_lgb)\n",
        "\n",
        "        # XGBoost\n",
        "        bst_xgb = xgb.Booster(model_file=f'D:\\\\MUFG Data Science Champion Ship 2023\\\\model\\\\separate models\\\\xgb_model_{transaction_type}_fold{fold_n + 1}.txt')\n",
        "        preds_xgb = bst_xgb.predict(xgb_test)\n",
        "        predictions_xgb.append(preds_xgb)\n",
        "\n",
        "    # Average the predictions from the two models\n",
        "    mean_preds_lgb = np.mean(predictions_lgb, axis=0)\n",
        "    mean_preds_xgb = np.mean(predictions_xgb, axis=0)\n",
        "    mean_preds_ensemble = (mean_preds_lgb + mean_preds_xgb) / 2\n",
        "\n",
        "    # Convert the predictions to binary\n",
        "    binary_predictions = (mean_preds_ensemble > mean_optimal_threshold).astype(int)\n",
        "\n",
        "    return df_test[df_test['use_chip'] == transaction_type]['index'].values, binary_predictions\n",
        "\n",
        "# Get predictions for each transaction type and concatenate\n",
        "transaction_types = ['Swipe Transaction', 'Chip Transaction', 'Online Transaction']\n",
        "all_ids = []\n",
        "all_predictions = []\n",
        "\n",
        "for transaction_type in transaction_types:\n",
        "    ids, preds = get_predictions_for_transaction_type(final_data, df_test, transaction_type)\n",
        "    all_ids.extend(ids)\n",
        "    all_predictions.extend(preds)\n",
        "\n",
        "# Create a dataframe for submission\n",
        "submission_df = pd.DataFrame({\n",
        "    'ID': all_ids,\n",
        "    'is_fraud?': all_predictions\n",
        "})\n",
        "\n",
        "# Sort the dataframe by ID to ensure the original order is maintained\n",
        "submission_df = submission_df.sort_values(by='ID')\n",
        "\n",
        "# Save to CSV without header and index\n",
        "submission_df.to_csv('D:\\\\MUFG Data Science Champion Ship 2023\\\\predictions\\\\submit11_Separatemodels.csv', index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcTgMnYcRVq-"
      },
      "source": [
        "# Adversarial Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ieISHTMRVq-"
      },
      "outputs": [],
      "source": [
        "# 1. カテゴリ変数の指定とデータ型の変換\n",
        "categorical_features = final_data.select_dtypes(include=['object']).columns.tolist()\n",
        "for col in categorical_features:\n",
        "    final_data[col] = final_data[col].astype('category')\n",
        "\n",
        "# 2. 'amount' カラムの値から `$` を削除し、浮動小数点数に変換\n",
        "# final_data['amount'] = final_data['amount'].str.replace('$', '').astype(float)\n",
        "\n",
        "# 3. 'index' カラムの削除\n",
        "# final_data.drop('index', axis=1, inplace=True)\n",
        "\n",
        "# 4. 目的変数 'is_fraud?' の削除\n",
        "# final_data.drop('is_fraud?', axis=1, inplace=True)\n",
        "\n",
        "# 5. 'is_test' カラムを使用して、学習データとテストデータに分割\n",
        "final_data_train = final_data[final_data['is_test'] == 0]\n",
        "final_data_test = final_data[final_data['is_test'] == 1]\n",
        "\n",
        "# 1. 学習データとテストデータの結合\n",
        "combined_data = pd.concat([final_data_train, final_data_test])\n",
        "\n",
        "# 目的変数と説明変数の定義\n",
        "y = combined_data['is_test']\n",
        "X = combined_data.drop('is_test', axis=1)\n",
        "\n",
        "# 学習データと検証データに分割\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# LightGBMのデータセットに変換\n",
        "train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=categorical_features, free_raw_data=False)\n",
        "val_data = lgb.Dataset(X_val, label=y_val, categorical_feature=categorical_features, free_raw_data=False)\n",
        "\n",
        "# パラメータの設定\n",
        "params = {\n",
        "    'objective': 'binary',\n",
        "    'metric': 'auc',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 31,\n",
        "    'learning_rate': 0.05,\n",
        "    'feature_fraction': 0.9\n",
        "}\n",
        "\n",
        "# モデルの訓練\n",
        "num_round = 1000\n",
        "bst = lgb.train(params, train_data, num_round, valid_sets=[val_data])\n",
        "\n",
        "# 検証データでの予測\n",
        "y_pred_val = bst.predict(X_val, num_iteration=bst.best_iteration)\n",
        "\n",
        "# AUCの計算\n",
        "auc_score = roc_auc_score(y_val, y_pred_val)\n",
        "auc_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEU5oW2URVq_"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Calculate the ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_val, y_pred_val)\n",
        "auc_score = auc(fpr, tpr)\n",
        "\n",
        "# Plotting the ROC curve\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % auc_score)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ipgJTYxRVq_"
      },
      "outputs": [],
      "source": [
        "# 特徴量の重要度をプロット\n",
        "lgb.plot_importance(bst, figsize=(10, 15))\n",
        "plt.title(\"Feature Importance\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AS86xi41RVq_"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "\n",
        "# Load the shap values for the trained model\n",
        "explainer = shap.TreeExplainer(bst)\n",
        "shap_values = explainer.shap_values(X_val)\n",
        "\n",
        "# Visualize the shap summary plot\n",
        "shap.summary_plot(shap_values, X_val)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzFej11HRVq_"
      },
      "source": [
        "# Cataboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDJBhSW5RVq_"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from catboost import CatBoostClassifier\n",
        "\n",
        "\n",
        "# # 金額のカラムを浮動小数点数に変換\n",
        "# df_train['amount'] = df_train['amount'].str.replace('$', '').astype(float)\n",
        "\n",
        "# # indexカラムの削除\n",
        "# df_train = df_train.drop(columns=['index'])\n",
        "\n",
        "# # カテゴリ変数のリスト\n",
        "# categorical_features = ['user_id', 'card_id', 'errors?', 'merchant_id', 'merchant_city', 'merchant_state', 'zip', 'mcc', 'use_chip']\n",
        "\n",
        "# # 欠損値処理: カテゴリ変数は 'Unknown' で埋める\n",
        "# df_train[categorical_features] = df_train[categorical_features].fillna('Unknown')\n",
        "# df_train['zip'] = df_train['zip'].astype(str)\n",
        "\n",
        "# # データセットを学習用と検証用に分割\n",
        "# X = df_train.drop(columns=['is_fraud?'])\n",
        "# y = df_train['is_fraud?']\n",
        "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# # CatBoostの学習\n",
        "# cat_model = CatBoostClassifier(iterations=1000, depth=6, learning_rate=0.1, loss_function='Logloss', verbose=200, cat_features=categorical_features)\n",
        "# cat_model.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=50)\n",
        "\n",
        "# # モデルの保存\n",
        "# # model_path = \"path_to_save_model.cbm\"  # モデルを保存するパスを設定してください\n",
        "# # cat_model.save_model(model_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70WhiFagRVq_"
      },
      "outputs": [],
      "source": [
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# 前処理\n",
        "df_train['amount'] = df_train['amount'].str.replace('$', '').astype(float)\n",
        "df_train = df_train.drop(columns=['index'])\n",
        "categorical_features_cat = ['card_id', 'errors?', 'merchant_id', 'merchant_city', 'merchant_state', 'zip', 'mcc', 'use_chip']\n",
        "df_train[categorical_features_cat] = df_train[categorical_features_cat].fillna('Unknown')\n",
        "df_train['zip'] = df_train['zip'].astype(str)\n",
        "\n",
        "X_cat = df_train.drop(columns=['is_fraud?'])\n",
        "y_cat = df_train['is_fraud?']\n",
        "\n",
        "f1_scores_cat = []\n",
        "\n",
        "folds_cat = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
        "\n",
        "for fold_n, (train_index_cat, valid_index_cat) in tqdm(enumerate(folds_cat.split(X_cat, y_cat))):\n",
        "    print(f'Fold {fold_n + 1} started at {time.ctime()}')\n",
        "    X_train_cat, X_val_cat = X_cat.iloc[train_index_cat], X_cat.iloc[valid_index_cat]\n",
        "    y_train_cat, y_val_cat = y_cat.iloc[train_index_cat], y_cat.iloc[valid_index_cat]\n",
        "\n",
        "    cat_model = CatBoostClassifier(iterations=10000, depth=6, learning_rate=0.1, loss_function='Logloss', verbose=200, cat_features=categorical_features_cat)\n",
        "    cat_model.fit(X_train_cat, y_train_cat, eval_set=(X_val_cat, y_val_cat), early_stopping_rounds=50)\n",
        "\n",
        "    y_pred_probs_cat = cat_model.predict_proba(X_val_cat)[:, 1]\n",
        "\n",
        "    # 最適なしきい値を適用してクラス分類\n",
        "    precision, recall, thresholds = precision_recall_curve(y_val_cat, y_pred_probs_cat)\n",
        "    f1_scores_thresholds = 2 * (precision * recall) / (precision + recall)\n",
        "    optimal_threshold = thresholds[np.argmax(f1_scores_thresholds)]\n",
        "    y_pred_binary = (y_pred_probs_cat > optimal_threshold).astype(int)\n",
        "\n",
        "    f1 = f1_score(y_val_cat, y_pred_binary)\n",
        "    f1_scores_cat.append(f1)\n",
        "\n",
        "    # モデルの保存\n",
        "    cat_model.save_model(f'D:\\\\MUFG Data Science Champion Ship 2023\\\\model\\\\cat_model_fold{fold_n + 1}.cbm')\n",
        "\n",
        "print('CatBoost CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(f1_scores_cat), np.std(f1_scores_cat))) #47m51.3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Go_GIQryRVrA"
      },
      "outputs": [],
      "source": [
        "# LightGBMとXGBoostの予測\n",
        "# 全データを使ってカテゴリカル変数をエンコード\n",
        "for col in categorical_features:\n",
        "    final_data[col] = final_data[col].astype('category')\n",
        "\n",
        "# 以降のコードはほとんど変わりませんが、lgb.Datasetを作成する際にfree_raw_dataを指定します\n",
        "train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=categorical_features, free_raw_data=False)\n",
        "valid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=categorical_features, reference=train_data, free_raw_data=False)\n",
        "\n",
        "\n",
        "y_pred_probs_lgb = bst_lgb.predict(X_val_cat) # ここでX_val_catを使います\n",
        "y_pred_probs_xgb = bst_xgb.predict(xgb.DMatrix(X_val_cat, enable_categorical=True))\n",
        "\n",
        "# CatBoostの予測\n",
        "y_pred_probs_cat = cat_model.predict_proba(X_val_cat)[:, 1]\n",
        "\n",
        "# 3つのモデルの予測確率を平均\n",
        "y_pred_probs_avg = (y_pred_probs_lgb + y_pred_probs_xgb + y_pred_probs_cat) / 3\n",
        "\n",
        "# 最適なしきい値を適用してクラス分類\n",
        "precision, recall, thresholds = precision_recall_curve(y_val_cat, y_pred_probs_avg) # ここでy_val_catを使います\n",
        "f1_scores_thresholds = 2 * (precision * recall) / (precision + recall)\n",
        "optimal_threshold = thresholds[np.argmax(f1_scores_thresholds)]\n",
        "y_pred_binary = (y_pred_probs_avg > optimal_threshold).astype(int)\n",
        "\n",
        "# F1スコアの計算\n",
        "f1 = f1_score(y_val_cat, y_pred_binary) # ここでy_val_catを使います\n",
        "print(f'F1 score for ensemble: {f1:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RYcNpZ4RVrA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Test data predictions\n",
        "predictions_lgb = []\n",
        "predictions_xgb = []\n",
        "predictions_cat = []\n",
        "\n",
        "# Get features for the test data\n",
        "final_data_test_features = final_data_test.drop(columns=['is_fraud?', 'is_test'])\n",
        "xgb_test = xgb.DMatrix(final_data_test_features, enable_categorical=True)\n",
        "\n",
        "# Get predictions for each model\n",
        "for fold_n in range(4):\n",
        "    # LightGBM\n",
        "    bst_lgb = lgb.Booster(model_file=f'D:\\\\MUFG Data Science Champion Ship 2023\\\\model\\\\lgb_model_fold{fold_n + 1}.txt')\n",
        "    preds_lgb = bst_lgb.predict(final_data_test_features, num_iteration=bst_lgb.best_iteration)\n",
        "    predictions_lgb.append(preds_lgb)\n",
        "\n",
        "    # XGBoost\n",
        "    bst_xgb = xgb.Booster(model_file=f'D:\\\\MUFG Data Science Champion Ship 2023\\\\model\\\\xgb_model_fold{fold_n + 1}.txt')\n",
        "    preds_xgb = bst_xgb.predict(xgb_test)\n",
        "    predictions_xgb.append(preds_xgb)\n",
        "\n",
        "    # CatBoost\n",
        "    cat_model = CatBoostClassifier()\n",
        "    cat_model.load_model(f'D:\\\\MUFG Data Science Champion Ship 2023\\\\model\\\\cat_model_fold{fold_n + 1}.cbm')\n",
        "    preds_cat = cat_model.predict_proba(final_data_test_features)[:, 1]\n",
        "    predictions_cat.append(preds_cat)\n",
        "\n",
        "# Get the average predictions\n",
        "mean_preds_lgb = np.mean(predictions_lgb, axis=0)\n",
        "mean_preds_xgb = np.mean(predictions_xgb, axis=0)\n",
        "mean_preds_cat = np.mean(predictions_cat, axis=0)\n",
        "\n",
        "# Average predictions from the three models\n",
        "mean_preds_ensemble = (mean_preds_lgb + mean_preds_xgb + mean_preds_cat) / 3\n",
        "\n",
        "# Get the average of the optimal thresholds (from LightGBM and XGBoost)\n",
        "mean_optimal_threshold = np.mean(optimal_thresholds)\n",
        "\n",
        "# Convert the predictions to binary\n",
        "binary_predictions = (mean_preds_ensemble > mean_optimal_threshold).astype(int)\n",
        "\n",
        "# Create a dataframe for submission\n",
        "submission_df = pd.DataFrame({\n",
        "    'ID': df_test['index'].values,\n",
        "    'is_fraud?': binary_predictions\n",
        "})\n",
        "\n",
        "# Save to CSV without header and index\n",
        "submission_df.to_csv('D:\\\\MUFG Data Science Champion Ship 2023\\\\predictions\\\\submit_baseline6.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GNkdpj5RVrA"
      },
      "source": [
        "# Lasso"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0acIw3JXRVrA"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "vir",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}